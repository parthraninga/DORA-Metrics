name: DORA Metrics Daily Ingestion

on:
  schedule:
    - cron: '0 2 * * *' # 02:00 UTC daily
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (log repos without calling API)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

concurrency:
  group: dora-ingestion
  cancel-in-progress: false # queue rather than cancel; prevents data gaps

jobs:
  ingest:
    name: Ingest DORA Metrics
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install @supabase/supabase-js@2

      - name: Run ingestion orchestration
        timeout-minutes: 58
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          DORA_API_URL: ${{ secrets.DORA_API_URL }}
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
        run: |
          node --input-type=module << 'EOF'
          import { createClient } from '@supabase/supabase-js';
          import { createHash }    from 'crypto';

          // ── Constants ──────────────────────────────────────────────────────────
          const SUPABASE_URL              = process.env.SUPABASE_URL;
          const SUPABASE_SERVICE_ROLE_KEY = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const DORA_API_URL              = process.env.DORA_API_URL;
          const DRY_RUN                   = process.env.DRY_RUN === 'true';

          const MAX_RETRIES            = 3;
          const RETRY_DELAY_MS         = 5_000;   // base delay; multiplied per attempt
          const SAFE_HISTORICAL_DAYS   = 90;       // fallback lookback when no last_fetched_at
          const RECENT_SKIP_MINUTES    = 5;        // skip repos fetched within this window
          const BATCH_SIZE             = 5;        // parallel concurrency per batch
          const PAGE_SIZE              = 1000;     // Supabase rows per page
          const REQUEST_TIMEOUT_MS     = 120_000;  // per-request timeout (2 min — longest observed success was 26s, heavy repos ~60-90s)

          // ── Structured logger ──────────────────────────────────────────────────
          const log = {
            info:  (msg, meta = {}) => console.log(JSON.stringify({ level: 'INFO',  ts: new Date().toISOString(), msg, ...meta })),
            warn:  (msg, meta = {}) => console.log(JSON.stringify({ level: 'WARN',  ts: new Date().toISOString(), msg, ...meta })),
            error: (msg, meta = {}) => console.error(JSON.stringify({ level: 'ERROR', ts: new Date().toISOString(), msg, ...meta })),
          };

          // ── Validate required secrets ──────────────────────────────────────────
          const missing = ['SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY', 'DORA_API_URL']
            .filter(k => !process.env[k]);
          if (missing.length) {
            log.error('Missing required secrets', { missing });
            process.exit(1);
          }

          if (DRY_RUN) log.warn('DRY RUN mode enabled — API calls will be skipped');

          // ── Supabase client ────────────────────────────────────────────────────
          const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, {
            auth: { persistSession: false },
          });

          // ── Helper: sleep ──────────────────────────────────────────────────────
          const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));

          // ── Helper: detect SCM provider from repo metadata ────────────────────────
          function detectProvider(repo) {
            const haystack = `${repo.org_name ?? ''} ${repo.repo_name ?? ''}`.toLowerCase();
            if (haystack.includes('github'))    return 'github';
            if (haystack.includes('bitbucket')) return 'bitbucket';
            if (haystack.includes('azure'))     return 'azure';
            if (haystack.includes('aws'))       return 'aws';
            if (haystack.includes('jenkins'))   return 'jenkins';
            return 'unknown';
          }

          // ── Helper: fetch ALL repos with pagination ────────────────────────────
          async function fetchAllRepos() {
            const allRepos = [];
            let from = 0;

            while (true) {
              const { data, error } = await supabase
                .from('repos')
                .select('id, repo_name, org_name, token_id, prod_branch, last_fetched_at, created_at')
                .order('last_fetched_at', { ascending: true, nullsFirst: true })
                .range(from, from + PAGE_SIZE - 1);

              if (error) {
                throw new Error(`Supabase pagination error at range ${from}: ${error.message}`);
              }

              if (!data || data.length === 0) break;

              allRepos.push(...data);
              log.info('Fetched page', { from, count: data.length, total_so_far: allRepos.length });

              if (data.length < PAGE_SIZE) break; // last page
              from += PAGE_SIZE;
            }

            return allRepos;
          }

          // ── Helper: fetch all tokens in ONE query, build in-memory cache ───────
          // Tokens are held only in memory and NEVER logged.
          async function fetchTokenCache(tokenIds) {
            const uniqueIds = [...new Set(tokenIds)].filter(Boolean);
            if (uniqueIds.length === 0) return new Map();

            const { data, error } = await supabase
              .from('tokens')
              .select('id, token')
              .in('id', uniqueIds);

            if (error) {
              throw new Error(`Failed to fetch tokens from Supabase: ${error.message}`);
            }

            const cache = new Map();
            for (const row of data ?? []) {
              cache.set(row.id, row.token);
            }

            // Log only counts — never token values
            log.info('Token cache built', {
              requested: uniqueIds.length,
              resolved:  cache.size,
              missing:   uniqueIds.length - cache.size,
            });

            return cache;
          }

          // ── Helper: generate deterministic UUID v5-like from a name ─────────────
          function generateDeterministicUUID(namespace, name) {
            const hash = createHash('sha1');
            hash.update(namespace + name);
            const hex = hash.digest('hex');
            return [
              hex.substring(0, 8),
              hex.substring(8, 12),
              '5' + hex.substring(13, 16),
              ((parseInt(hex.substring(16, 18), 16) & 0x3f) | 0x80).toString(16) + hex.substring(18, 20),
              hex.substring(20, 32),
            ].join('-');
          }

          // ── Helper: map PR from Lambda response to DB schema ──────────────────
          function mapPullRequest(pr, repoId, fetchDataId) {
            if (!pr.id) return null;
            return {
              id:                    pr.id,
              repo_id:               repoId,
              fetch_data_id:         fetchDataId,
              pr_no:                 pr.number?.toString() ?? pr.pr_no?.toString(),
              title:                 pr.title,
              author:                typeof pr.author === 'object' ? pr.author?.username : pr.author,
              first_commit_to_open:  pr.first_commit_to_open,
              cycle_time:            pr.cycle_time,
              created_at:            pr.created_at,
              updated_at:            pr.updated_at,
              state:                 pr.state,
              base_branch:           pr.base_branch,
              head_branch:           pr.head_branch,
              commits:               pr.commits,
              additions:             pr.additions,
              deletions:             pr.deletions,
              comments:              pr.comments,
              url:                   pr.url,
              state_changed_at:      pr.state_changed_at,
              provider:              pr.provider,
              reviewers:             pr.reviewers ? JSON.stringify(pr.reviewers) : null,
              lead_time:             pr.lead_time,
              merge_time:            pr.merge_time,
              deploy_time:           pr.deploy_time,
              first_response_time:   pr.first_response_time,
              rework_time:           pr.rework_time,
              merge_to_deploy:       pr.merge_to_deploy,
            };
          }

          // ── Helper: map incident from Lambda response to DB schema ───────────
          function mapIncident(incident, repoId, fetchDataId) {
            const validUuid = v => {
              if (!v || v === 'None' || v === 'null' || v === 'undefined') return null;
              return /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(v) ? v : null;
            };
            let id = validUuid(incident.id);
            if (!id) {
              if (!incident.key) return null;
              id = generateDeterministicUUID('incident', incident.key);
            }
            let workflowRunId = null;
            if (incident.key?.startsWith('workflow-')) {
              const n = incident.key.replace('workflow-', '');
              if (n && !isNaN(n)) workflowRunId = parseInt(n, 10);
            }
            return {
              id,
              repo_id:          repoId,
              fetch_data_id:    fetchDataId,
              pull_request_id:  validUuid(incident.pull_request_id),
              workflow_run_id:  workflowRunId,
              pr_no:            incident.incident_number?.toString(),
              creation_date:    incident.creation_date,
              resolved_date:    incident.resolved_date,
              created_at:       incident.creation_date,
              title:            incident.title,
              status:           incident.status?.replace('IncidentStatus.', ''),
              incident_type:    incident.incident_type,
              url:              incident.url,
              provider:         incident.provider?.replace('IncidentProvider.', ''),
              summary:          incident.summary,
              assigned_to:      incident.assigned_to?.username,
            };
          }

          // ── Helper: map workflow run from Lambda response to DB schema ───────
          function mapWorkflowRun(run, repoId, fetchDataId) {
            if (!run.id) return null;
            let runId = run.id;
            if (!/^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(runId.toString())) {
              runId = generateDeterministicUUID('workflow_run', runId.toString());
            }
            return {
              id:            runId,
              repo_id:       repoId,
              fetch_data_id: fetchDataId,
              run_id:        (run.run_id ?? run.id)?.toString(),
              name:          run.name ?? run.workflow_name,
              workflow_name: run.workflow_name ?? run.name,
              workflow_file: run.workflow_file,
              head_branch:   run.head_branch,
              status:        run.status,
              conclusion:    run.conclusion,
              created_at:    run.created_at,
              updated_at:    run.updated_at,
              html_url:      run.html_url ?? run.url,
              actor:         typeof run.actor === 'object' ? run.actor?.username : run.actor,
              workflow_id:   run.workflow_id?.toString(),
            };
          }

          // ── Helper: persist Lambda response body to Supabase ────────────────
          async function persistRepoData(repo, responseBody, fetchedAt) {
            const provider = detectProvider(repo);

            // 1. Insert fetch_data record
            const { data: fetchRecord, error: fetchErr } = await supabase
              .from('fetch_data')
              .insert({ repo_id: repo.id, fetched_at: fetchedAt, state: 'success', raw_response: responseBody })
              .select('id')
              .single();

            if (fetchErr) {
              log.error('Failed to create fetch_data record', { repo_id: repo.id, provider, error: fetchErr.message });
              return { success: false };
            }

            const fetchDataId    = fetchRecord.id;
            let totalPRs         = 0;
            let totalIncidents   = 0;
            let totalWorkflowRuns = 0;

            for (const repoData of (responseBody.repos ?? [])) {
              if (repoData.error) continue; // repo failed inside Lambda — skip

              // Pull Requests
              if (repoData.pull_requests?.length > 0) {
                const prs = repoData.pull_requests.map(pr => mapPullRequest(pr, repo.id, fetchDataId)).filter(Boolean);
                if (prs.length > 0) {
                  const { error } = await supabase.from('pull_requests').upsert(prs, { onConflict: 'id', ignoreDuplicates: false });
                  if (error) log.error('PR upsert error', { repo_id: repo.id, provider, error: error.message });
                  else totalPRs += prs.length;
                }
              }

              // Incidents
              if (repoData.incidents?.length > 0) {
                const incidents = repoData.incidents.map(inc => mapIncident(inc, repo.id, fetchDataId)).filter(Boolean);
                if (incidents.length > 0) {
                  const { error } = await supabase.from('incidents').upsert(incidents, { onConflict: 'id', ignoreDuplicates: false });
                  if (error) log.error('Incidents upsert error', { repo_id: repo.id, provider, error: error.message });
                  else totalIncidents += incidents.length;
                }
              }

              // Workflow Runs
              if (repoData.workflow_runs?.length > 0) {
                const runs = repoData.workflow_runs.map(run => mapWorkflowRun(run, repo.id, fetchDataId)).filter(Boolean);
                if (runs.length > 0) {
                  const { error } = await supabase.from('workflow_runs').upsert(runs, { onConflict: 'id', ignoreDuplicates: false });
                  if (error) log.error('Workflow runs upsert error', { repo_id: repo.id, provider, error: error.message });
                  else totalWorkflowRuns += runs.length;
                }
              }
            }

            // 2. Update repos.last_fetched_at
            const { error: repoUpdateErr } = await supabase
              .from('repos')
              .update({ last_fetched_at: fetchedAt })
              .eq('id', repo.id);
            if (repoUpdateErr) {
              log.warn('Failed to update repos.last_fetched_at', { repo_id: repo.id, error: repoUpdateErr.message });
            }

            log.info('Persisted repo data to Supabase', {
              repo_id:              repo.id,
              provider,
              fetch_data_id:        fetchDataId,
              total_prs:            totalPRs,
              total_incidents:      totalIncidents,
              total_workflow_runs:  totalWorkflowRuns,
            });

            return { success: true };
          }

          // ── Helper: call ingestion API with retry + exponential back-off ───────
          async function callIngestionApi(repo, fromTime, toTime, githubPatToken) {
            const provider = detectProvider(repo);
            const payload = {
              github_pat_token: githubPatToken,
              repos: [
                {
                  org_name: repo.org_name,
                  repo_name: repo.repo_name
                }
              ],
              from_time: fromTime,
              to_time: toTime,
            };
            let lastError;
            const apiCallStart = Date.now();

            // Patterns for GitHub permission / credential failures (case-insensitive)
            const GITHUB_PERM_PATTERNS   = ['bad credentials', 'resource not accessible', 'requires authentication', 'not found', 'must have push access'];
            const GITHUB_EXPIRY_PATTERNS = ['token expired', 'revoked', 'invalid token'];

            for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
              const controller = new AbortController();
              const timeoutId  = setTimeout(() => controller.abort(), REQUEST_TIMEOUT_MS);

              try {
                const res = await fetch(DORA_API_URL, {
                  method:  'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body:    JSON.stringify(payload),
                  signal:  controller.signal,
                });

                clearTimeout(timeoutId);

                // ── Extract Lambda / proxy request ID (never throws if header absent) ──
                const lambdaRequestId =
                  res.headers.get('x-amzn-requestid') ??
                  res.headers.get('x-request-id')     ??
                  res.headers.get('lambda-request-id') ??
                  null;
                if (lambdaRequestId) {
                  log.info('Lambda request id', { repo_id: repo.id, provider, request_id: lambdaRequestId });
                }

                if (res.ok) {
                  const body        = await res.json().catch(() => ({}));
                  const duration_ms = Date.now() - apiCallStart;
                  const bodyText    = JSON.stringify(body).toLowerCase();

                  // Detect permission / expiry signals even on 2xx responses
                  if (GITHUB_PERM_PATTERNS.some(p => bodyText.includes(p))) {
                    log.error('GitHub permission issue detected', { repo_id: repo.id, provider });
                  }
                  if (GITHUB_EXPIRY_PATTERNS.some(p => bodyText.includes(p))) {
                    log.error('GitHub token expired', { repo_id: repo.id, provider });
                  }

                  log.info('Ingestion API success', {
                    repo_id:      repo.id,
                    repo_name:    repo.repo_name,
                    org_name:     repo.org_name,
                    provider,
                    from_time:    fromTime,
                    to_time:      toTime,
                    status:       res.status,
                    attempt,
                    duration_ms,
                  });
                  return { success: true, body };
                }

                const errBody  = await res.text().catch(() => '');
                lastError      = { status: res.status, body: errBody };
                const errLower = errBody.toLowerCase();

                if (GITHUB_PERM_PATTERNS.some(p => errLower.includes(p))) {
                  log.error('GitHub permission issue detected', { repo_id: repo.id, provider });
                }
                if (GITHUB_EXPIRY_PATTERNS.some(p => errLower.includes(p))) {
                  log.error('GitHub token expired', { repo_id: repo.id, provider });
                }

                log.warn('Ingestion API non-200', {
                  repo_id:     repo.id,
                  repo_name:   repo.repo_name,
                  provider,
                  status:      res.status,
                  body:        errBody,
                  attempt,
                  max_retries: MAX_RETRIES,
                });

              } catch (err) {
                clearTimeout(timeoutId);
                lastError = { message: err.message };
                log.warn('Ingestion API request error', {
                  repo_id:     repo.id,
                  repo_name:   repo.repo_name,
                  provider,
                  error:       err.message,
                  attempt,
                  max_retries: MAX_RETRIES,
                });
              }

              if (attempt < MAX_RETRIES) {
                const delay = RETRY_DELAY_MS * attempt; // 5s, 10s, 15s
                log.info('Retrying', { repo_id: repo.id, provider, delay_ms: delay, next_attempt: attempt + 1 });
                await sleep(delay);
              }
            }

            log.error('Ingestion API failed after all retries', {
              repo_id:    repo.id,
              repo_name:  repo.repo_name,
              org_name:   repo.org_name,
              provider,
              last_error: lastError,
            });

            return { success: false, error: lastError };
          }

          // ── Helper: process one repo ───────────────────────────────────────────
          async function processRepo(repo, toTime, runStart, tokenCache) {
            const provider = detectProvider(repo);

            log.info('Processing repo', {
              repo_id:         repo.id,
              repo_name:       repo.repo_name,
              org_name:        repo.org_name,
              provider,
              last_fetched_at: repo.last_fetched_at ?? null,
            });

            // Skip repos fetched very recently to avoid duplicate ingestion
            if (repo.last_fetched_at) {
              const minutesSinceFetch = (runStart - new Date(repo.last_fetched_at)) / 60_000;
              if (minutesSinceFetch < RECENT_SKIP_MINUTES) {
                log.warn('Skipping repo — fetched too recently', {
                  repo_id:             repo.id,
                  repo_name:           repo.repo_name,
                  provider,
                  minutes_since_fetch: minutesSinceFetch.toFixed(2),
                  threshold_minutes:   RECENT_SKIP_MINUTES,
                });
                return { status: 'skipped' };
              }
            }

            // Derive from_time
            let fromTime;
            if (repo.last_fetched_at) {
              const from = new Date(repo.last_fetched_at);
              from.setSeconds(from.getSeconds() - 60); // 60s overlap to avoid boundary gaps
              fromTime = from.toISOString();
            } else if (repo.created_at) {
              fromTime = repo.created_at;
            } else {
              const fallback = new Date(runStart);
              fallback.setDate(fallback.getDate() - SAFE_HISTORICAL_DAYS);
              fromTime = fallback.toISOString();
            }

            log.info('Time window', { repo_id: repo.id, provider, from_time: fromTime, to_time: toTime });

            // Resolve token from cache — skip repo if token not found
            const githubPatToken = tokenCache.get(repo.token_id);
            if (!githubPatToken) {
              log.error('Token not found in cache — skipping repo', {
                repo_id:   repo.id,
                repo_name: repo.repo_name,
                provider,
                token_id:  repo.token_id ?? null,
              });
              return { status: 'failed', error: { message: 'token_not_found' } };
            }

            if (DRY_RUN) {
              log.info('DRY RUN: skipping API call', { repo_id: repo.id, provider });
              return { status: 'dry_run' };
            }

            const repoStart   = Date.now();
            const result      = await callIngestionApi(repo, fromTime, toTime, githubPatToken);
            const duration_ms = Date.now() - repoStart;

            log.info('Repo ingestion finished', { repo_id: repo.id, provider, duration_ms });

            if (!result.success) {
              return { status: 'failed', error: result.error };
            }

            // Persist Lambda response body to Supabase
            const persisted = await persistRepoData(repo, result.body, toTime);
            if (!persisted.success) {
              return { status: 'failed', error: { message: 'persist_failed' } };
            }

            return { status: 'success' };
          }

          // ── Helper: run array of tasks in bounded parallel batches ─────────────
          async function runInBatches(items, batchSize, taskFn) {
            const results = [];
            for (let i = 0; i < items.length; i += batchSize) {
              const batch = items.slice(i, i + batchSize);
              log.info('Starting batch', {
                batch_index: Math.floor(i / batchSize) + 1,
                batch_size:  batch.length,
                total_items: items.length,
              });
              const batchResults = await Promise.all(batch.map(item => taskFn(item)));
              results.push(...batchResults);
            }
            return results;
          }

          // ── Main ───────────────────────────────────────────────────────────────
          async function main() {
            const runStart = new Date();
            log.info('DORA ingestion run started', {
              triggered_by: process.env.GITHUB_EVENT_NAME ?? 'unknown',
              run_id:       process.env.GITHUB_RUN_ID ?? 'local',
              dry_run:      DRY_RUN,
            });

            // Paginate through all repos
            let repos;
            try {
              repos = await fetchAllRepos();
            } catch (err) {
              log.error('Failed to fetch repos from Supabase', { error: err.message });
              process.exit(1);
            }

            if (!repos.length) {
              log.warn('No repos found in database — nothing to ingest');
              process.exit(0);
            }

            log.info('Fetched all repos', { total: repos.length });

            // Build token cache with a single Supabase query
            let tokenCache;
            try {
              const tokenIds = repos.map(r => r.token_id).filter(Boolean);
              tokenCache = await fetchTokenCache(tokenIds);
            } catch (err) {
              log.error('Failed to build token cache', { error: err.message });
              process.exit(1);
            }

            const toTime   = runStart.toISOString();
            const counters = { success: 0, failed: 0, skipped: 0, dry_run: 0 };
            const failures = [];

            const repoResults = await runInBatches(repos, BATCH_SIZE, (repo) =>
              processRepo(repo, toTime, runStart, tokenCache).then(result => ({ repo, result }))
            );

            for (const { repo, result } of repoResults) {
              counters[result.status] = (counters[result.status] ?? 0) + 1;
              if (result.status === 'failed') {
                failures.push({ repo_id: repo.id, repo_name: repo.repo_name, error: result.error });
              }
            }

            const durationMs = Date.now() - runStart.getTime();
            log.info('DORA run summary', {
              total_repos:       repos.length,
              success_count:     counters.success  ?? 0,
              failed_count:      counters.failed   ?? 0,
              skipped_count:     counters.skipped  ?? 0,
              dry_run_count:     counters.dry_run  ?? 0,
              total_duration_ms: durationMs,
            });

            if (failures.length > 0) {
              log.error('Some repos failed ingestion', { failures });
              process.exit(1);
            }
          }

          await main();
          EOF

      - name: Report outcome
        if: always()
        run: |
          if [ "${{ job.status }}" = "success" ]; then
            echo "::notice title=DORA Ingestion::Run completed successfully"
          else
            echo "::error title=DORA Ingestion::Run failed — check logs above"
          fi
