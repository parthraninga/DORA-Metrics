name: DORA Metrics Daily Ingestion

on:
  schedule:
    - cron: '0 2 * * *' # 02:00 UTC daily
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (log repos without calling API)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

concurrency:
  group: dora-ingestion
  cancel-in-progress: false # queue rather than cancel; prevents data gaps

jobs:
  ingest:
    name: Ingest DORA Metrics
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install @supabase/supabase-js@2

      - name: Run ingestion orchestration
        timeout-minutes: 28
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          DORA_API_URL: ${{ secrets.DORA_API_URL }}
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
        run: |
          node --input-type=module << 'EOF'
          import { createClient } from '@supabase/supabase-js';

          // ── Constants ──────────────────────────────────────────────────────────
          const SUPABASE_URL              = process.env.SUPABASE_URL;
          const SUPABASE_SERVICE_ROLE_KEY = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const DORA_API_URL              = process.env.DORA_API_URL;
          const DRY_RUN                   = process.env.DRY_RUN === 'true';

          const MAX_RETRIES            = 3;
          const RETRY_DELAY_MS         = 5_000;   // base delay; multiplied per attempt
          const SAFE_HISTORICAL_DAYS   = 90;       // fallback lookback when no last_fetched_at
          const RECENT_SKIP_MINUTES    = 5;        // skip repos fetched within this window
          const BATCH_SIZE             = 5;        // parallel concurrency per batch
          const PAGE_SIZE              = 1000;     // Supabase rows per page
          const REQUEST_TIMEOUT_MS     = 30_000;   // per-request timeout

          // ── Structured logger ──────────────────────────────────────────────────
          const log = {
            info:  (msg, meta = {}) => console.log(JSON.stringify({ level: 'INFO',  ts: new Date().toISOString(), msg, ...meta })),
            warn:  (msg, meta = {}) => console.log(JSON.stringify({ level: 'WARN',  ts: new Date().toISOString(), msg, ...meta })),
            error: (msg, meta = {}) => console.error(JSON.stringify({ level: 'ERROR', ts: new Date().toISOString(), msg, ...meta })),
          };

          // ── Validate required secrets ──────────────────────────────────────────
          const missing = ['SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY', 'DORA_API_URL']
            .filter(k => !process.env[k]);
          if (missing.length) {
            log.error('Missing required secrets', { missing });
            process.exit(1);
          }

          if (DRY_RUN) log.warn('DRY RUN mode enabled — API calls will be skipped');

          // ── Supabase client ────────────────────────────────────────────────────
          const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, {
            auth: { persistSession: false },
          });

          // ── Helper: sleep ──────────────────────────────────────────────────────
          const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));

          // ── Helper: fetch ALL repos with pagination ────────────────────────────
          async function fetchAllRepos() {
            const allRepos = [];
            let from = 0;

            while (true) {
              const { data, error } = await supabase
                .from('repos')
                .select('id, repo_name, org_name, prod_branch, last_fetched_at, created_at')
                .order('last_fetched_at', { ascending: true, nullsFirst: true })
                .range(from, from + PAGE_SIZE - 1);

              if (error) {
                throw new Error(`Supabase pagination error at range ${from}: ${error.message}`);
              }

              if (!data || data.length === 0) break;

              allRepos.push(...data);
              log.info('Fetched page', { from, count: data.length, total_so_far: allRepos.length });

              if (data.length < PAGE_SIZE) break; // last page
              from += PAGE_SIZE;
            }

            return allRepos;
          }

          // ── Helper: call ingestion API with retry + exponential back-off ───────
          async function callIngestionApi(repo, fromTime, toTime) {
            const payload = { repo_id: repo.id, from_time: fromTime, to_time: toTime };
            let lastError;

            for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
              const controller = new AbortController();
              const timeoutId  = setTimeout(() => controller.abort(), REQUEST_TIMEOUT_MS);

              try {
                const res = await fetch(DORA_API_URL, {
                  method:  'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body:    JSON.stringify(payload),
                  signal:  controller.signal,
                });

                clearTimeout(timeoutId);

                if (res.ok) {
                  const body = await res.json().catch(() => ({}));
                  log.info('Ingestion API success', {
                    repo_id:   repo.id,
                    repo_name: repo.repo_name,
                    org_name:  repo.org_name,
                    from_time: fromTime,
                    to_time:   toTime,
                    status:    res.status,
                    attempt,
                    response:  body,
                  });
                  return { success: true };
                }

                const errBody = await res.text().catch(() => '');
                lastError = { status: res.status, body: errBody };
                log.warn('Ingestion API non-200', {
                  repo_id:     repo.id,
                  repo_name:   repo.repo_name,
                  status:      res.status,
                  body:        errBody,
                  attempt,
                  max_retries: MAX_RETRIES,
                });

              } catch (err) {
                clearTimeout(timeoutId);
                lastError = { message: err.message };
                log.warn('Ingestion API request error', {
                  repo_id:     repo.id,
                  repo_name:   repo.repo_name,
                  error:       err.message,
                  attempt,
                  max_retries: MAX_RETRIES,
                });
              }

              if (attempt < MAX_RETRIES) {
                const delay = RETRY_DELAY_MS * attempt; // 5s, 10s, 15s
                log.info('Retrying', { repo_id: repo.id, delay_ms: delay, next_attempt: attempt + 1 });
                await sleep(delay);
              }
            }

            log.error('Ingestion API failed after all retries', {
              repo_id:    repo.id,
              repo_name:  repo.repo_name,
              org_name:   repo.org_name,
              last_error: lastError,
            });

            return { success: false, error: lastError };
          }

          // ── Helper: process one repo ───────────────────────────────────────────
          async function processRepo(repo, toTime, runStart) {
            log.info('Processing repo', {
              repo_id:         repo.id,
              repo_name:       repo.repo_name,
              org_name:        repo.org_name,
              last_fetched_at: repo.last_fetched_at ?? null,
            });

            // Skip repos fetched very recently to avoid duplicate ingestion
            if (repo.last_fetched_at) {
              const minutesSinceFetch = (runStart - new Date(repo.last_fetched_at)) / 60_000;
              if (minutesSinceFetch < RECENT_SKIP_MINUTES) {
                log.warn('Skipping repo — fetched too recently', {
                  repo_id:             repo.id,
                  repo_name:           repo.repo_name,
                  minutes_since_fetch: minutesSinceFetch.toFixed(2),
                  threshold_minutes:   RECENT_SKIP_MINUTES,
                });
                return { status: 'skipped' };
              }
            }

            // Derive from_time
            let fromTime;
            if (repo.last_fetched_at) {
              const from = new Date(repo.last_fetched_at);
              from.setSeconds(from.getSeconds() - 60); // 60s overlap to avoid boundary gaps
              fromTime = from.toISOString();
            } else if (repo.created_at) {
              fromTime = repo.created_at;
            } else {
              const fallback = new Date(runStart);
              fallback.setDate(fallback.getDate() - SAFE_HISTORICAL_DAYS);
              fromTime = fallback.toISOString();
            }

            log.info('Time window', { repo_id: repo.id, from_time: fromTime, to_time: toTime });

            if (DRY_RUN) {
              log.info('DRY RUN: skipping API call', { repo_id: repo.id });
              return { status: 'dry_run' };
            }

            const result = await callIngestionApi(repo, fromTime, toTime);
            return result.success
              ? { status: 'success' }
              : { status: 'failed', error: result.error };
          }

          // ── Helper: run array of tasks in bounded parallel batches ─────────────
          async function runInBatches(items, batchSize, taskFn) {
            const results = [];
            for (let i = 0; i < items.length; i += batchSize) {
              const batch = items.slice(i, i + batchSize);
              log.info('Starting batch', {
                batch_index: Math.floor(i / batchSize) + 1,
                batch_size:  batch.length,
                total_items: items.length,
              });
              const batchResults = await Promise.all(batch.map(item => taskFn(item)));
              results.push(...batchResults);
            }
            return results;
          }

          // ── Main ───────────────────────────────────────────────────────────────
          async function main() {
            const runStart = new Date();
            log.info('DORA ingestion run started', {
              triggered_by: process.env.GITHUB_EVENT_NAME ?? 'unknown',
              run_id:       process.env.GITHUB_RUN_ID ?? 'local',
              dry_run:      DRY_RUN,
            });

            // Paginate through all repos
            let repos;
            try {
              repos = await fetchAllRepos();
            } catch (err) {
              log.error('Failed to fetch repos from Supabase', { error: err.message });
              process.exit(1);
            }

            if (!repos.length) {
              log.warn('No repos found in database — nothing to ingest');
              process.exit(0);
            }

            log.info('Fetched all repos', { total: repos.length });

            const toTime   = runStart.toISOString();
            const counters = { success: 0, failed: 0, skipped: 0, dry_run: 0 };
            const failures = [];

            const repoResults = await runInBatches(repos, BATCH_SIZE, (repo) =>
              processRepo(repo, toTime, runStart).then(result => ({ repo, result }))
            );

            for (const { repo, result } of repoResults) {
              counters[result.status] = (counters[result.status] ?? 0) + 1;
              if (result.status === 'failed') {
                failures.push({ repo_id: repo.id, repo_name: repo.repo_name, error: result.error });
              }
            }

            const durationMs = Date.now() - runStart.getTime();
            log.info('DORA ingestion run complete', {
              total:       repos.length,
              ...counters,
              duration_ms: durationMs,
            });

            if (failures.length > 0) {
              log.error('Some repos failed ingestion', { failures });
              process.exit(1);
            }
          }

          await main();
          EOF

      - name: Report outcome
        if: always()
        run: |
          if [ "${{ job.status }}" = "success" ]; then
            echo "::notice title=DORA Ingestion::Run completed successfully"
          else
            echo "::error title=DORA Ingestion::Run failed — check logs above"
          fi
