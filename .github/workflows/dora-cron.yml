name: DORA Metrics Daily Ingestion

on:
  schedule:
    - cron: '0 2 * * *' # 02:00 UTC daily
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (log repos without calling API)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

concurrency:
  group: dora-ingestion
  cancel-in-progress: false # queue rather than cancel; prevents data gaps

jobs:
  ingest:
    name: Ingest DORA Metrics
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install @supabase/supabase-js@2

      - name: Run ingestion orchestration
        timeout-minutes: 58
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          DORA_API_URL: ${{ secrets.DORA_API_URL }}
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
        run: |
          node --input-type=module << 'EOF'
          import { createClient } from '@supabase/supabase-js';
          import { createHash }    from 'crypto';

          // ── Constants ──────────────────────────────────────────────────────────
          const SUPABASE_URL              = process.env.SUPABASE_URL;
          const SUPABASE_SERVICE_ROLE_KEY = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const DORA_API_URL              = process.env.DORA_API_URL;
          const DRY_RUN                   = process.env.DRY_RUN === 'true';

          const MAX_RETRIES            = 3;
          const RETRY_DELAY_MS         = 5_000;   // base delay; multiplied per attempt
          const SAFE_HISTORICAL_DAYS   = 90;       // fallback lookback when no last_fetched_at
          const RECENT_SKIP_MINUTES    = 5;        // skip repos fetched within this window
          const BATCH_SIZE             = 5;        // parallel concurrency per batch
          const PAGE_SIZE              = 1000;     // Supabase rows per page
          const REQUEST_TIMEOUT_MS     = 120_000;  // per-request timeout (2 min — longest observed success was 26s, heavy repos ~60-90s)

          // ── Structured logger ──────────────────────────────────────────────────
          const log = {
            info:  (msg, meta = {}) => console.log(JSON.stringify({ level: 'INFO',  ts: new Date().toISOString(), msg, ...meta })),
            warn:  (msg, meta = {}) => console.log(JSON.stringify({ level: 'WARN',  ts: new Date().toISOString(), msg, ...meta })),
            error: (msg, meta = {}) => console.error(JSON.stringify({ level: 'ERROR', ts: new Date().toISOString(), msg, ...meta })),
          };

          // ── Validate required secrets ──────────────────────────────────────────
          const missing = ['SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY', 'DORA_API_URL']
            .filter(k => !process.env[k]);
          if (missing.length) {
            log.error('Missing required secrets', { missing });
            process.exit(1);
          }

          if (DRY_RUN) log.warn('DRY RUN mode enabled — API calls will be skipped');

          // ── Supabase client ────────────────────────────────────────────────────
          const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, {
            auth: { persistSession: false },
          });

          // ── Helper: sleep ──────────────────────────────────────────────────────
          const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));

          // ── Helper: detect SCM provider from repo metadata ────────────────────────
          function detectProvider(repo) {
            const haystack = `${repo.org_name ?? ''} ${repo.repo_name ?? ''}`.toLowerCase();
            if (haystack.includes('github'))    return 'github';
            if (haystack.includes('bitbucket')) return 'bitbucket';
            if (haystack.includes('azure'))     return 'azure';
            if (haystack.includes('aws'))       return 'aws';
            if (haystack.includes('jenkins'))   return 'jenkins';
            return 'unknown';
          }

          // ── Helper: fetch ALL repos with pagination ────────────────────────────
          async function fetchAllRepos() {
            const allRepos = [];
            let from = 0;

            while (true) {
              const { data, error } = await supabase
                .from('repos')
                .select('id, repo_name, org_name, token_id, prod_branch, last_fetched_at, created_at')
                .order('last_fetched_at', { ascending: true, nullsFirst: true })
                .range(from, from + PAGE_SIZE - 1);

              if (error) {
                throw new Error(`Supabase pagination error at range ${from}: ${error.message}`);
              }

              if (!data || data.length === 0) break;

              allRepos.push(...data);
              log.info('Fetched page', { from, count: data.length, total_so_far: allRepos.length });

              if (data.length < PAGE_SIZE) break; // last page
              from += PAGE_SIZE;
            }

            return allRepos;
          }

          // ── Helper: fetch all tokens in ONE query, build in-memory cache ───────
          // Tokens are held only in memory and NEVER logged.
          async function fetchTokenCache(tokenIds) {
            const uniqueIds = [...new Set(tokenIds)].filter(Boolean);
            if (uniqueIds.length === 0) return new Map();

            const { data, error } = await supabase
              .from('tokens')
              .select('id, token')
              .in('id', uniqueIds);

            if (error) {
              throw new Error(`Failed to fetch tokens from Supabase: ${error.message}`);
            }

            const cache = new Map();
            for (const row of data ?? []) {
              cache.set(row.id, row.token);
            }

            // Log only counts — never token values
            log.info('Token cache built', {
              requested: uniqueIds.length,
              resolved:  cache.size,
              missing:   uniqueIds.length - cache.size,
            });

            return cache;
          }

          // ── Helper: generate deterministic UUID v5-like from a name ─────────────
          function generateDeterministicUUID(namespace, name) {
            const hash = createHash('sha1');
            hash.update(namespace + name);
            const hex = hash.digest('hex');
            return [
              hex.substring(0, 8),
              hex.substring(8, 12),
              '5' + hex.substring(13, 16),
              ((parseInt(hex.substring(16, 18), 16) & 0x3f) | 0x80).toString(16) + hex.substring(18, 20),
              hex.substring(20, 32),
            ].join('-');
          }

          // ── Shared helpers used inside persistRepoData ────────────────────────
          const toTs  = s => s == null ? null : typeof s === 'string' ? s : typeof s === 'number' ? new Date(s).toISOString() : null;
          const toInt = v => { if (v == null) return null; const n = typeof v === 'number' ? v : parseInt(String(v), 10); return Number.isFinite(n) ? n : null; };
          const toAuthor = a => {
            if (a == null) return null;
            if (typeof a === 'string') return a;
            if (typeof a === 'object') return a.username ?? a.login ?? null;
            return null;
          };

          // ── Helper: persist Lambda response body to Supabase ────────────────
          // Actual Lambda response shape (confirmed via parse-fetch-response.ts):
          //   { repos: [ { deployments: [ { related_prs: [...] } ], workflow_runs: [...] } ] }
          //   OR wrapped: { data: { repos: [...] } }
          // PRs live inside deployments[].related_prs — NOT at repos[].pull_requests.
          // PRs have no UUID id; DB auto-generates it (use insert, not upsert).
          // Incidents are derived from workflow_run failures, not a direct field.
          async function persistRepoData(repo, responseBody, fetchedAt) {
            const provider = detectProvider(repo);

            // 1. Insert fetch_data record
            const { data: fetchRecord, error: fetchErr } = await supabase
              .from('fetch_data')
              .insert({ repo_id: repo.id, fetched_at: fetchedAt, state: 'success', raw_response: responseBody })
              .select('id')
              .single();

            if (fetchErr) {
              log.error('Failed to create fetch_data record', { repo_id: repo.id, provider, error: fetchErr.message });
              return { success: false };
            }

            const fetchDataId     = fetchRecord.id;
            let totalPRs          = 0;
            let totalIncidents    = 0;
            let totalWorkflowRuns = 0;

            // Normalise root: handle both { repos:[...] } and { data:{ repos:[...] } }
            const root  = responseBody ?? {};
            const data  = root.data ?? root;
            const repos = Array.isArray(data.repos) ? data.repos : [];

            // ── Pull Requests ─────────────────────────────────────────────────
            // PRs are nested: repos[].deployments[].related_prs[]
            const seenPrNo = new Set();
            for (const repoData of repos) {
              if (repoData.error) continue;
              const deployments = Array.isArray(repoData.deployments) ? repoData.deployments : [];
              for (const dep of deployments) {
                const prs = Array.isArray(dep.related_prs) ? dep.related_prs : [];
                for (const pr of prs) {
                  const no = pr.no ?? pr.number;
                  if (no == null) continue;
                  const prNo = typeof no === 'number' ? no : parseInt(String(no), 10);
                  if (isNaN(prNo) || seenPrNo.has(prNo)) continue;
                  seenPrNo.add(prNo);

                  // id is auto-generated by the DB — do NOT include it
                  const { error } = await supabase.from('pull_requests').insert({
                    repo_id:              repo.id,
                    fetch_data_id:        fetchDataId,
                    pr_no:                prNo,
                    title:                pr.title ?? null,
                    author:               toAuthor(pr.author),
                    first_commit_to_open: pr.first_commit_to_open ?? null,
                    cycle_time:           pr.cycle_time ?? null,
                    created_at:           toTs(pr.created_at),
                    updated_at:           toTs(pr.updated_at),
                    state:                pr.state ?? null,
                    base_branch:          typeof pr.base_branch === 'string' ? pr.base_branch : null,
                    head_branch:          typeof pr.head_branch === 'string' ? pr.head_branch : null,
                    commits:              toInt(pr.commits),
                    additions:            toInt(pr.additions),
                    deletions:            toInt(pr.deletions),
                    comments:             toInt(pr.comments),
                  });
                  if (!error) totalPRs++;
                  else log.error('PR insert error', { repo_id: repo.id, provider, pr_no: prNo, error: error.message });
                }
              }
            }

            // ── Workflow Runs ─────────────────────────────────────────────────
            // repos[].workflow_runs[] plus optional top-level data.workflow_runs[]
            const workflowRunsRaw = [];
            for (const repoData of repos) {
              if (repoData.error) continue;
              if (Array.isArray(repoData.workflow_runs)) workflowRunsRaw.push(...repoData.workflow_runs);
            }
            if (Array.isArray(data.workflow_runs)) workflowRunsRaw.push(...data.workflow_runs);

            for (const wr of workflowRunsRaw) {
              const rawId = wr.id ?? wr.run_id;
              if (rawId == null) continue;
              const rawIdStr = String(rawId);
              const isUUID   = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(rawIdStr);
              const uuidId   = isUUID ? rawIdStr : generateDeterministicUUID('workflow_run', rawIdStr);

              const { error } = await supabase.from('workflow_runs').upsert({
                id:            uuidId,
                repo_id:       repo.id,
                fetch_data_id: fetchDataId,
                run_id:        rawIdStr,
                name:          wr.name ?? wr.workflow_name ?? null,
                workflow_name: wr.workflow_name ?? wr.name ?? null,
                workflow_file: wr.workflow_file ?? null,
                head_branch:   wr.head_branch ?? null,
                status:        wr.status ?? null,
                conclusion:    wr.conclusion ?? null,
                created_at:    toTs(wr.created_at),
                updated_at:    toTs(wr.updated_at),
                html_url:      wr.html_url ?? wr.url ?? null,
                actor:         toAuthor(wr.actor),
                workflow_id:   wr.workflow_id?.toString() ?? null,
              }, { onConflict: 'id', ignoreDuplicates: false });
              if (!error) totalWorkflowRuns++;
              else log.error('Workflow run upsert error', { repo_id: repo.id, provider, run_id: rawIdStr, error: error.message });
            }

            // ── Incidents (derived from workflow_run failures) ─────────────────
            // A failed run = incident creation; next success run = resolved date.
            const runsWithMeta = [];
            for (const wr of workflowRunsRaw) {
              const rawId  = wr.id ?? wr.run_id;
              const created = toTs(wr.created_at);
              if (rawId == null || created == null) continue;
              const id = typeof rawId === 'number' ? rawId : parseInt(String(rawId), 10);
              if (isNaN(id)) continue;
              runsWithMeta.push({ runId: id, created_at: created, conclusion: typeof wr.conclusion === 'string' ? wr.conclusion : null });
            }
            runsWithMeta.sort((a, b) => a.created_at.localeCompare(b.created_at));

            for (let i = 0; i < runsWithMeta.length; i++) {
              const run = runsWithMeta[i];
              if ((run.conclusion ?? '').toLowerCase() !== 'failure') continue;
              let resolvedDate = null;
              for (let j = i + 1; j < runsWithMeta.length; j++) {
                if ((runsWithMeta[j].conclusion ?? '').toLowerCase() === 'success') {
                  resolvedDate = runsWithMeta[j].created_at;
                  break;
                }
              }
              const { error } = await supabase.from('incidents').insert({
                repo_id:         repo.id,
                fetch_data_id:   fetchDataId,
                pull_request_id: null,
                workflow_run_id: run.runId,
                pr_no:           null,
                creation_date:   run.created_at,
                resolved_date:   resolvedDate,
              });
              if (!error) totalIncidents++;
              else log.error('Incident insert error', { repo_id: repo.id, provider, error: error.message });
            }

            // 2. Update repos.last_fetched_at
            const { error: repoUpdateErr } = await supabase
              .from('repos')
              .update({ last_fetched_at: fetchedAt })
              .eq('id', repo.id);
            if (repoUpdateErr) {
              log.warn('Failed to update repos.last_fetched_at', { repo_id: repo.id, error: repoUpdateErr.message });
            }

            log.info('Persisted repo data to Supabase', {
              repo_id:              repo.id,
              provider,
              fetch_data_id:        fetchDataId,
              total_prs:            totalPRs,
              total_incidents:      totalIncidents,
              total_workflow_runs:  totalWorkflowRuns,
            });

            return { success: true };
          }

          // ── Helper: call ingestion API with retry + exponential back-off ───────
          async function callIngestionApi(repo, fromTime, toTime, githubPatToken) {
            const provider = detectProvider(repo);
            const payload = {
              github_pat_token: githubPatToken,
              repos: [
                {
                  org_name: repo.org_name,
                  repo_name: repo.repo_name
                }
              ],
              from_time: fromTime,
              to_time: toTime,
            };
            let lastError;
            const apiCallStart = Date.now();

            // Patterns for GitHub permission / credential failures (case-insensitive)
            const GITHUB_PERM_PATTERNS   = ['bad credentials', 'resource not accessible', 'requires authentication', 'not found', 'must have push access'];
            const GITHUB_EXPIRY_PATTERNS = ['token expired', 'revoked', 'invalid token'];

            for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
              const controller = new AbortController();
              const timeoutId  = setTimeout(() => controller.abort(), REQUEST_TIMEOUT_MS);

              try {
                const res = await fetch(DORA_API_URL, {
                  method:  'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body:    JSON.stringify(payload),
                  signal:  controller.signal,
                });

                clearTimeout(timeoutId);

                // ── Extract Lambda / proxy request ID (never throws if header absent) ──
                const lambdaRequestId =
                  res.headers.get('x-amzn-requestid') ??
                  res.headers.get('x-request-id')     ??
                  res.headers.get('lambda-request-id') ??
                  null;
                if (lambdaRequestId) {
                  log.info('Lambda request id', { repo_id: repo.id, provider, request_id: lambdaRequestId });
                }

                if (res.ok) {
                  const body        = await res.json().catch(() => ({}));
                  const duration_ms = Date.now() - apiCallStart;
                  const bodyText    = JSON.stringify(body).toLowerCase();

                  // Detect permission / expiry signals even on 2xx responses
                  if (GITHUB_PERM_PATTERNS.some(p => bodyText.includes(p))) {
                    log.error('GitHub permission issue detected', { repo_id: repo.id, provider });
                  }
                  if (GITHUB_EXPIRY_PATTERNS.some(p => bodyText.includes(p))) {
                    log.error('GitHub token expired', { repo_id: repo.id, provider });
                  }

                  log.info('Ingestion API success', {
                    repo_id:      repo.id,
                    repo_name:    repo.repo_name,
                    org_name:     repo.org_name,
                    provider,
                    from_time:    fromTime,
                    to_time:      toTime,
                    status:       res.status,
                    attempt,
                    duration_ms,
                  });
                  return { success: true, body };
                }

                const errBody  = await res.text().catch(() => '');
                lastError      = { status: res.status, body: errBody };
                const errLower = errBody.toLowerCase();

                if (GITHUB_PERM_PATTERNS.some(p => errLower.includes(p))) {
                  log.error('GitHub permission issue detected', { repo_id: repo.id, provider });
                }
                if (GITHUB_EXPIRY_PATTERNS.some(p => errLower.includes(p))) {
                  log.error('GitHub token expired', { repo_id: repo.id, provider });
                }

                log.warn('Ingestion API non-200', {
                  repo_id:     repo.id,
                  repo_name:   repo.repo_name,
                  provider,
                  status:      res.status,
                  body:        errBody,
                  attempt,
                  max_retries: MAX_RETRIES,
                });

              } catch (err) {
                clearTimeout(timeoutId);
                lastError = { message: err.message };
                log.warn('Ingestion API request error', {
                  repo_id:     repo.id,
                  repo_name:   repo.repo_name,
                  provider,
                  error:       err.message,
                  attempt,
                  max_retries: MAX_RETRIES,
                });
              }

              if (attempt < MAX_RETRIES) {
                const delay = RETRY_DELAY_MS * attempt; // 5s, 10s, 15s
                log.info('Retrying', { repo_id: repo.id, provider, delay_ms: delay, next_attempt: attempt + 1 });
                await sleep(delay);
              }
            }

            log.error('Ingestion API failed after all retries', {
              repo_id:    repo.id,
              repo_name:  repo.repo_name,
              org_name:   repo.org_name,
              provider,
              last_error: lastError,
            });

            return { success: false, error: lastError };
          }

          // ── Helper: process one repo ───────────────────────────────────────────
          async function processRepo(repo, toTime, runStart, tokenCache) {
            const provider = detectProvider(repo);

            log.info('Processing repo', {
              repo_id:         repo.id,
              repo_name:       repo.repo_name,
              org_name:        repo.org_name,
              provider,
              last_fetched_at: repo.last_fetched_at ?? null,
            });

            // Skip repos fetched very recently to avoid duplicate ingestion
            if (repo.last_fetched_at) {
              const minutesSinceFetch = (runStart - new Date(repo.last_fetched_at)) / 60_000;
              if (minutesSinceFetch < RECENT_SKIP_MINUTES) {
                log.warn('Skipping repo — fetched too recently', {
                  repo_id:             repo.id,
                  repo_name:           repo.repo_name,
                  provider,
                  minutes_since_fetch: minutesSinceFetch.toFixed(2),
                  threshold_minutes:   RECENT_SKIP_MINUTES,
                });
                return { status: 'skipped' };
              }
            }

            // Derive from_time
            let fromTime;
            if (repo.last_fetched_at) {
              const from = new Date(repo.last_fetched_at);
              from.setSeconds(from.getSeconds() - 60); // 60s overlap to avoid boundary gaps
              fromTime = from.toISOString();
            } else if (repo.created_at) {
              fromTime = repo.created_at;
            } else {
              const fallback = new Date(runStart);
              fallback.setDate(fallback.getDate() - SAFE_HISTORICAL_DAYS);
              fromTime = fallback.toISOString();
            }

            log.info('Time window', { repo_id: repo.id, provider, from_time: fromTime, to_time: toTime });

            // Resolve token from cache — skip repo if token not found
            const githubPatToken = tokenCache.get(repo.token_id);
            if (!githubPatToken) {
              log.error('Token not found in cache — skipping repo', {
                repo_id:   repo.id,
                repo_name: repo.repo_name,
                provider,
                token_id:  repo.token_id ?? null,
              });
              return { status: 'failed', error: { message: 'token_not_found' } };
            }

            if (DRY_RUN) {
              log.info('DRY RUN: skipping API call', { repo_id: repo.id, provider });
              return { status: 'dry_run' };
            }

            const repoStart   = Date.now();
            const result      = await callIngestionApi(repo, fromTime, toTime, githubPatToken);
            const duration_ms = Date.now() - repoStart;

            log.info('Repo ingestion finished', { repo_id: repo.id, provider, duration_ms });

            if (!result.success) {
              return { status: 'failed', error: result.error };
            }

            // Persist Lambda response body to Supabase
            const persisted = await persistRepoData(repo, result.body, toTime);
            if (!persisted.success) {
              return { status: 'failed', error: { message: 'persist_failed' } };
            }

            return { status: 'success' };
          }

          // ── Helper: run array of tasks in bounded parallel batches ─────────────
          async function runInBatches(items, batchSize, taskFn) {
            const results = [];
            for (let i = 0; i < items.length; i += batchSize) {
              const batch = items.slice(i, i + batchSize);
              log.info('Starting batch', {
                batch_index: Math.floor(i / batchSize) + 1,
                batch_size:  batch.length,
                total_items: items.length,
              });
              const batchResults = await Promise.all(batch.map(item => taskFn(item)));
              results.push(...batchResults);
            }
            return results;
          }

          // ── Main ───────────────────────────────────────────────────────────────
          async function main() {
            const runStart = new Date();
            log.info('DORA ingestion run started', {
              triggered_by: process.env.GITHUB_EVENT_NAME ?? 'unknown',
              run_id:       process.env.GITHUB_RUN_ID ?? 'local',
              dry_run:      DRY_RUN,
            });

            // Paginate through all repos
            let repos;
            try {
              repos = await fetchAllRepos();
            } catch (err) {
              log.error('Failed to fetch repos from Supabase', { error: err.message });
              process.exit(1);
            }

            if (!repos.length) {
              log.warn('No repos found in database — nothing to ingest');
              process.exit(0);
            }

            log.info('Fetched all repos', { total: repos.length });

            // Build token cache with a single Supabase query
            let tokenCache;
            try {
              const tokenIds = repos.map(r => r.token_id).filter(Boolean);
              tokenCache = await fetchTokenCache(tokenIds);
            } catch (err) {
              log.error('Failed to build token cache', { error: err.message });
              process.exit(1);
            }

            const toTime   = runStart.toISOString();
            const counters = { success: 0, failed: 0, skipped: 0, dry_run: 0 };
            const failures = [];

            const repoResults = await runInBatches(repos, BATCH_SIZE, (repo) =>
              processRepo(repo, toTime, runStart, tokenCache).then(result => ({ repo, result }))
            );

            for (const { repo, result } of repoResults) {
              counters[result.status] = (counters[result.status] ?? 0) + 1;
              if (result.status === 'failed') {
                failures.push({ repo_id: repo.id, repo_name: repo.repo_name, error: result.error });
              }
            }

            const durationMs = Date.now() - runStart.getTime();
            log.info('DORA run summary', {
              total_repos:       repos.length,
              success_count:     counters.success  ?? 0,
              failed_count:      counters.failed   ?? 0,
              skipped_count:     counters.skipped  ?? 0,
              dry_run_count:     counters.dry_run  ?? 0,
              total_duration_ms: durationMs,
            });

            if (failures.length > 0) {
              log.error('Some repos failed ingestion', { failures });
              process.exit(1);
            }
          }

          await main();
          EOF

      - name: Report outcome
        if: always()
        run: |
          if [ "${{ job.status }}" = "success" ]; then
            echo "::notice title=DORA Ingestion::Run completed successfully"
          else
            echo "::error title=DORA Ingestion::Run failed — check logs above"
          fi
