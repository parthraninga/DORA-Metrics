name: DORA Metrics Daily Ingestion

on:
  schedule:
    - cron: '0 2 * * *' # 02:00 UTC daily
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (log repos without calling API)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

concurrency:
  group: dora-ingestion
  cancel-in-progress: false # queue rather than cancel; prevents data gaps

jobs:
  ingest:
    name: Ingest DORA Metrics
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install @supabase/supabase-js@2

      - name: Run ingestion orchestration
        timeout-minutes: 58
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          DORA_API_URL: ${{ secrets.DORA_API_URL }}
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
        run: |
          node --input-type=module << 'EOF'
          import { createClient } from '@supabase/supabase-js';

          // ── Constants ──────────────────────────────────────────────────────────
          const SUPABASE_URL              = process.env.SUPABASE_URL;
          const SUPABASE_SERVICE_ROLE_KEY = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const DORA_API_URL              = process.env.DORA_API_URL;
          const DRY_RUN                   = process.env.DRY_RUN === 'true';

          const MAX_RETRIES            = 3;
          const RETRY_DELAY_MS         = 5_000;   // base delay; multiplied per attempt
          const SAFE_HISTORICAL_DAYS   = 90;       // fallback lookback when no last_fetched_at
          const RECENT_SKIP_MINUTES    = 5;        // skip repos fetched within this window
          const BATCH_SIZE             = 5;        // parallel concurrency per batch
          const PAGE_SIZE              = 1000;     // Supabase rows per page
          const REQUEST_TIMEOUT_MS     = 120_000;  // per-request timeout (2 min — longest observed success was 26s, heavy repos ~60-90s)

          // ── Structured logger ──────────────────────────────────────────────────
          const log = {
            info:  (msg, meta = {}) => console.log(JSON.stringify({ level: 'INFO',  ts: new Date().toISOString(), msg, ...meta })),
            warn:  (msg, meta = {}) => console.log(JSON.stringify({ level: 'WARN',  ts: new Date().toISOString(), msg, ...meta })),
            error: (msg, meta = {}) => console.error(JSON.stringify({ level: 'ERROR', ts: new Date().toISOString(), msg, ...meta })),
          };

          // ── Validate required secrets ──────────────────────────────────────────
          const missing = ['SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY', 'DORA_API_URL']
            .filter(k => !process.env[k]);
          if (missing.length) {
            log.error('Missing required secrets', { missing });
            process.exit(1);
          }

          if (DRY_RUN) log.warn('DRY RUN mode enabled — API calls will be skipped');

          // ── Supabase client ────────────────────────────────────────────────────
          const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, {
            auth: { persistSession: false },
          });

          // ── Helper: sleep ──────────────────────────────────────────────────────
          const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));

          // ── Helper: detect SCM provider from repo metadata ────────────────────────
          function detectProvider(repo) {
            const haystack = `${repo.org_name ?? ''} ${repo.repo_name ?? ''}`.toLowerCase();
            if (haystack.includes('github'))    return 'github';
            if (haystack.includes('bitbucket')) return 'bitbucket';
            if (haystack.includes('azure'))     return 'azure';
            if (haystack.includes('aws'))       return 'aws';
            if (haystack.includes('jenkins'))   return 'jenkins';
            return 'unknown';
          }

          // ── Helper: fetch ALL repos with pagination ────────────────────────────
          async function fetchAllRepos() {
            const allRepos = [];
            let from = 0;

            while (true) {
              const { data, error } = await supabase
                .from('repos')
                .select('id, repo_name, org_name, token_id, prod_branch, last_fetched_at, created_at')
                .order('last_fetched_at', { ascending: true, nullsFirst: true })
                .range(from, from + PAGE_SIZE - 1);

              if (error) {
                throw new Error(`Supabase pagination error at range ${from}: ${error.message}`);
              }

              if (!data || data.length === 0) break;

              allRepos.push(...data);
              log.info('Fetched page', { from, count: data.length, total_so_far: allRepos.length });

              if (data.length < PAGE_SIZE) break; // last page
              from += PAGE_SIZE;
            }

            return allRepos;
          }

          // ── Helper: fetch all tokens in ONE query, build in-memory cache ───────
          // Tokens are held only in memory and NEVER logged.
          async function fetchTokenCache(tokenIds) {
            const uniqueIds = [...new Set(tokenIds)].filter(Boolean);
            if (uniqueIds.length === 0) return new Map();

            const { data, error } = await supabase
              .from('tokens')
              .select('id, token')
              .in('id', uniqueIds);

            if (error) {
              throw new Error(`Failed to fetch tokens from Supabase: ${error.message}`);
            }

            const cache = new Map();
            for (const row of data ?? []) {
              cache.set(row.id, row.token);
            }

            // Log only counts — never token values
            log.info('Token cache built', {
              requested: uniqueIds.length,
              resolved:  cache.size,
              missing:   uniqueIds.length - cache.size,
            });

            return cache;
          }

          // ── Helper: call ingestion API with retry + exponential back-off ───────
          async function callIngestionApi(repo, fromTime, toTime, githubPatToken) {
            const provider = detectProvider(repo);
            const payload = {
              github_pat_token: githubPatToken,
              repos: [
                {
                  org_name: repo.org_name,
                  repo_name: repo.repo_name
                }
              ],
              from_time: fromTime,
              to_time: toTime,
            };
            let lastError;
            const apiCallStart = Date.now();

            // Patterns for GitHub permission / credential failures (case-insensitive)
            const GITHUB_PERM_PATTERNS   = ['bad credentials', 'resource not accessible', 'requires authentication', 'not found', 'must have push access'];
            const GITHUB_EXPIRY_PATTERNS = ['token expired', 'revoked', 'invalid token'];

            for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
              const controller = new AbortController();
              const timeoutId  = setTimeout(() => controller.abort(), REQUEST_TIMEOUT_MS);

              try {
                const res = await fetch(DORA_API_URL, {
                  method:  'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body:    JSON.stringify(payload),
                  signal:  controller.signal,
                });

                clearTimeout(timeoutId);

                // ── Extract Lambda / proxy request ID (never throws if header absent) ──
                const lambdaRequestId =
                  res.headers.get('x-amzn-requestid') ??
                  res.headers.get('x-request-id')     ??
                  res.headers.get('lambda-request-id') ??
                  null;
                if (lambdaRequestId) {
                  log.info('Lambda request id', { repo_id: repo.id, provider, request_id: lambdaRequestId });
                }

                if (res.ok) {
                  const body        = await res.json().catch(() => ({}));
                  const duration_ms = Date.now() - apiCallStart;
                  const bodyText    = JSON.stringify(body).toLowerCase();

                  // Detect permission / expiry signals even on 2xx responses
                  if (GITHUB_PERM_PATTERNS.some(p => bodyText.includes(p))) {
                    log.error('GitHub permission issue detected', { repo_id: repo.id, provider });
                  }
                  if (GITHUB_EXPIRY_PATTERNS.some(p => bodyText.includes(p))) {
                    log.error('GitHub token expired', { repo_id: repo.id, provider });
                  }

                  log.info('Ingestion API success', {
                    repo_id:      repo.id,
                    repo_name:    repo.repo_name,
                    org_name:     repo.org_name,
                    provider,
                    from_time:    fromTime,
                    to_time:      toTime,
                    status:       res.status,
                    attempt,
                    duration_ms,
                    response:     body,
                  });
                  return { success: true };
                }

                const errBody  = await res.text().catch(() => '');
                lastError      = { status: res.status, body: errBody };
                const errLower = errBody.toLowerCase();

                if (GITHUB_PERM_PATTERNS.some(p => errLower.includes(p))) {
                  log.error('GitHub permission issue detected', { repo_id: repo.id, provider });
                }
                if (GITHUB_EXPIRY_PATTERNS.some(p => errLower.includes(p))) {
                  log.error('GitHub token expired', { repo_id: repo.id, provider });
                }

                log.warn('Ingestion API non-200', {
                  repo_id:     repo.id,
                  repo_name:   repo.repo_name,
                  provider,
                  status:      res.status,
                  body:        errBody,
                  attempt,
                  max_retries: MAX_RETRIES,
                });

              } catch (err) {
                clearTimeout(timeoutId);
                lastError = { message: err.message };
                log.warn('Ingestion API request error', {
                  repo_id:     repo.id,
                  repo_name:   repo.repo_name,
                  provider,
                  error:       err.message,
                  attempt,
                  max_retries: MAX_RETRIES,
                });
              }

              if (attempt < MAX_RETRIES) {
                const delay = RETRY_DELAY_MS * attempt; // 5s, 10s, 15s
                log.info('Retrying', { repo_id: repo.id, provider, delay_ms: delay, next_attempt: attempt + 1 });
                await sleep(delay);
              }
            }

            log.error('Ingestion API failed after all retries', {
              repo_id:    repo.id,
              repo_name:  repo.repo_name,
              org_name:   repo.org_name,
              provider,
              last_error: lastError,
            });

            return { success: false, error: lastError };
          }

          // ── Helper: process one repo ───────────────────────────────────────────
          async function processRepo(repo, toTime, runStart, tokenCache) {
            const provider = detectProvider(repo);

            log.info('Processing repo', {
              repo_id:         repo.id,
              repo_name:       repo.repo_name,
              org_name:        repo.org_name,
              provider,
              last_fetched_at: repo.last_fetched_at ?? null,
            });

            // Skip repos fetched very recently to avoid duplicate ingestion
            if (repo.last_fetched_at) {
              const minutesSinceFetch = (runStart - new Date(repo.last_fetched_at)) / 60_000;
              if (minutesSinceFetch < RECENT_SKIP_MINUTES) {
                log.warn('Skipping repo — fetched too recently', {
                  repo_id:             repo.id,
                  repo_name:           repo.repo_name,
                  provider,
                  minutes_since_fetch: minutesSinceFetch.toFixed(2),
                  threshold_minutes:   RECENT_SKIP_MINUTES,
                });
                return { status: 'skipped' };
              }
            }

            // Derive from_time
            let fromTime;
            if (repo.last_fetched_at) {
              const from = new Date(repo.last_fetched_at);
              from.setSeconds(from.getSeconds() - 60); // 60s overlap to avoid boundary gaps
              fromTime = from.toISOString();
            } else if (repo.created_at) {
              fromTime = repo.created_at;
            } else {
              const fallback = new Date(runStart);
              fallback.setDate(fallback.getDate() - SAFE_HISTORICAL_DAYS);
              fromTime = fallback.toISOString();
            }

            log.info('Time window', { repo_id: repo.id, provider, from_time: fromTime, to_time: toTime });

            // Resolve token from cache — skip repo if token not found
            const githubPatToken = tokenCache.get(repo.token_id);
            if (!githubPatToken) {
              log.error('Token not found in cache — skipping repo', {
                repo_id:   repo.id,
                repo_name: repo.repo_name,
                provider,
                token_id:  repo.token_id ?? null,
              });
              return { status: 'failed', error: { message: 'token_not_found' } };
            }

            if (DRY_RUN) {
              log.info('DRY RUN: skipping API call', { repo_id: repo.id, provider });
              return { status: 'dry_run' };
            }

            const repoStart   = Date.now();
            const result      = await callIngestionApi(repo, fromTime, toTime, githubPatToken);
            const duration_ms = Date.now() - repoStart;

            log.info('Repo ingestion finished', { repo_id: repo.id, provider, duration_ms });

            return result.success
              ? { status: 'success' }
              : { status: 'failed', error: result.error };
          }

          // ── Helper: run array of tasks in bounded parallel batches ─────────────
          async function runInBatches(items, batchSize, taskFn) {
            const results = [];
            for (let i = 0; i < items.length; i += batchSize) {
              const batch = items.slice(i, i + batchSize);
              log.info('Starting batch', {
                batch_index: Math.floor(i / batchSize) + 1,
                batch_size:  batch.length,
                total_items: items.length,
              });
              const batchResults = await Promise.all(batch.map(item => taskFn(item)));
              results.push(...batchResults);
            }
            return results;
          }

          // ── Main ───────────────────────────────────────────────────────────────
          async function main() {
            const runStart = new Date();
            log.info('DORA ingestion run started', {
              triggered_by: process.env.GITHUB_EVENT_NAME ?? 'unknown',
              run_id:       process.env.GITHUB_RUN_ID ?? 'local',
              dry_run:      DRY_RUN,
            });

            // Paginate through all repos
            let repos;
            try {
              repos = await fetchAllRepos();
            } catch (err) {
              log.error('Failed to fetch repos from Supabase', { error: err.message });
              process.exit(1);
            }

            if (!repos.length) {
              log.warn('No repos found in database — nothing to ingest');
              process.exit(0);
            }

            log.info('Fetched all repos', { total: repos.length });

            // Build token cache with a single Supabase query
            let tokenCache;
            try {
              const tokenIds = repos.map(r => r.token_id).filter(Boolean);
              tokenCache = await fetchTokenCache(tokenIds);
            } catch (err) {
              log.error('Failed to build token cache', { error: err.message });
              process.exit(1);
            }

            const toTime   = runStart.toISOString();
            const counters = { success: 0, failed: 0, skipped: 0, dry_run: 0 };
            const failures = [];

            const repoResults = await runInBatches(repos, BATCH_SIZE, (repo) =>
              processRepo(repo, toTime, runStart, tokenCache).then(result => ({ repo, result }))
            );

            for (const { repo, result } of repoResults) {
              counters[result.status] = (counters[result.status] ?? 0) + 1;
              if (result.status === 'failed') {
                failures.push({ repo_id: repo.id, repo_name: repo.repo_name, error: result.error });
              }
            }

            const durationMs = Date.now() - runStart.getTime();
            log.info('DORA run summary', {
              total_repos:       repos.length,
              success_count:     counters.success  ?? 0,
              failed_count:      counters.failed   ?? 0,
              skipped_count:     counters.skipped  ?? 0,
              dry_run_count:     counters.dry_run  ?? 0,
              total_duration_ms: durationMs,
            });

            if (failures.length > 0) {
              log.error('Some repos failed ingestion', { failures });
              process.exit(1);
            }
          }

          await main();
          EOF

      - name: Report outcome
        if: always()
        run: |
          if [ "${{ job.status }}" = "success" ]; then
            echo "::notice title=DORA Ingestion::Run completed successfully"
          else
            echo "::error title=DORA Ingestion::Run failed — check logs above"
          fi
